{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "mount_file_id": "1hCmlHb7BXrZhwlHtNkyUVjsya-lvYWiM",
      "authorship_tag": "ABX9TyMKq9U71Dk2xwlVOv9OfH6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzs0153/ELEC-7970-Reinforcement-Learning/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teqhdm7AqhNo"
      },
      "source": [
        "By using Deep Q-Networks (DQN), we will solve an Open AI Gym environement 'Cartpole-v1': https://gym.openai.com/envs/CartPole-v1/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ZQ3Xq7qT7L"
      },
      "source": [
        "This code is adapted from https://keras.io/examples/rl/deep_q_network_breakout/.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id6jZUiSbP69",
        "outputId": "9c4b5e88-3642-4f58-9d10-b6de8fcd14bf"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor \n",
        "epsilon = 0.1  # Epsilon greedy parameter\n",
        "batch_size = 32  # Size of batch taken from replay buffer\n",
        "env =gym.make('CartPole-v1')\n",
        "env.seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6zXrKwY2aJA"
      },
      "source": [
        "num_actions = env.action_space.n\n",
        "\n",
        "def create_q_model():\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(4,))\n",
        "    layer1= layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    output = layers.Dense(num_actions, activation=\"linear\")(layer1)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "model = create_q_model()\n",
        "# Build a target model \n",
        "model_target = create_q_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqdMyixYDaXZ",
        "outputId": "324b4866-1178-4495-80fe-c61b9decdb3f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2560      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 3,586\n",
            "Trainable params: 3,586\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqnriwbTDnnj",
        "outputId": "13103f61-a40d-4cce-8275-fe53df2f980f"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "time_count = 0\n",
        "\n",
        "max_memory_length = 10000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 1000\n",
        "# huber loss \n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "    Termination= False\n",
        "    while Termination != True:\n",
        "        # env.render(); Adding this line would show the attempts\n",
        "        # of the agent in a pop up window.\n",
        "        time_count += 1\n",
        "\n",
        "        # Use epsilon-greedy for exploration\n",
        "        if epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if time_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor(\n",
        "                [float(done_history[i]) for i in indices]\n",
        "            )\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model \n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
        "                future_rewards, axis=1\n",
        "            )\n",
        "\n",
        "            # If final frame set the last value to 0\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) \n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on the states and updated Q-values\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                # Calculate loss between new Q-value and old Q-value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if time_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, time count {}\"\n",
        "            print(template.format(running_reward, episode_count, time_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            Termination=True\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    if running_reward > 190:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 10.05 at episode 99, time count 1000\n",
            "running reward: 10.19 at episode 197, time count 2000\n",
            "running reward: 9.80 at episode 299, time count 3000\n",
            "running reward: 9.69 at episode 402, time count 4000\n",
            "running reward: 10.35 at episode 499, time count 5000\n",
            "running reward: 9.88 at episode 600, time count 6000\n",
            "running reward: 9.93 at episode 701, time count 7000\n",
            "running reward: 10.27 at episode 798, time count 8000\n",
            "running reward: 11.20 at episode 887, time count 9000\n",
            "running reward: 11.88 at episode 970, time count 10000\n",
            "running reward: 13.11 at episode 1046, time count 11000\n",
            "running reward: 14.13 at episode 1118, time count 12000\n",
            "running reward: 13.35 at episode 1195, time count 13000\n",
            "running reward: 12.70 at episode 1276, time count 14000\n",
            "running reward: 13.86 at episode 1348, time count 15000\n",
            "running reward: 16.91 at episode 1401, time count 16000\n",
            "running reward: 20.44 at episode 1441, time count 17000\n",
            "running reward: 27.58 at episode 1465, time count 18000\n",
            "running reward: 33.84 at episode 1486, time count 19000\n",
            "running reward: 39.32 at episode 1503, time count 20000\n",
            "running reward: 47.67 at episode 1516, time count 21000\n",
            "running reward: 55.09 at episode 1527, time count 22000\n",
            "running reward: 62.01 at episode 1536, time count 23000\n",
            "running reward: 67.84 at episode 1544, time count 24000\n",
            "running reward: 74.78 at episode 1554, time count 25000\n",
            "running reward: 80.68 at episode 1564, time count 26000\n",
            "running reward: 86.23 at episode 1575, time count 27000\n",
            "running reward: 89.68 at episode 1587, time count 28000\n",
            "running reward: 90.31 at episode 1603, time count 29000\n",
            "running reward: 87.44 at episode 1620, time count 30000\n",
            "running reward: 77.85 at episode 1638, time count 31000\n",
            "running reward: 63.21 at episode 1661, time count 32000\n",
            "running reward: 53.93 at episode 1682, time count 33000\n",
            "running reward: 47.85 at episode 1706, time count 34000\n",
            "running reward: 43.46 at episode 1731, time count 35000\n",
            "running reward: 43.53 at episode 1753, time count 36000\n",
            "running reward: 41.25 at episode 1780, time count 37000\n",
            "running reward: 38.82 at episode 1809, time count 38000\n",
            "running reward: 37.22 at episode 1837, time count 39000\n",
            "running reward: 34.77 at episode 1868, time count 40000\n",
            "running reward: 37.49 at episode 1890, time count 41000\n",
            "running reward: 39.26 at episode 1912, time count 42000\n",
            "running reward: 41.97 at episode 1933, time count 43000\n",
            "running reward: 46.56 at episode 1950, time count 44000\n",
            "running reward: 50.05 at episode 1967, time count 45000\n",
            "running reward: 53.58 at episode 1984, time count 46000\n",
            "running reward: 55.99 at episode 2000, time count 47000\n",
            "running reward: 59.49 at episode 2013, time count 48000\n",
            "running reward: 63.37 at episode 2026, time count 49000\n",
            "running reward: 66.77 at episode 2039, time count 50000\n",
            "running reward: 69.06 at episode 2052, time count 51000\n",
            "running reward: 71.89 at episode 2064, time count 52000\n",
            "running reward: 73.86 at episode 2077, time count 53000\n",
            "running reward: 76.26 at episode 2089, time count 54000\n",
            "running reward: 78.98 at episode 2101, time count 55000\n",
            "running reward: 79.90 at episode 2113, time count 56000\n",
            "running reward: 81.78 at episode 2123, time count 57000\n",
            "running reward: 84.67 at episode 2133, time count 58000\n",
            "running reward: 86.55 at episode 2143, time count 59000\n",
            "running reward: 89.70 at episode 2152, time count 60000\n",
            "running reward: 93.66 at episode 2159, time count 61000\n",
            "running reward: 99.12 at episode 2166, time count 62000\n",
            "running reward: 103.69 at episode 2171, time count 63000\n",
            "running reward: 109.77 at episode 2178, time count 64000\n",
            "running reward: 114.77 at episode 2183, time count 65000\n",
            "running reward: 120.81 at episode 2189, time count 66000\n",
            "running reward: 125.18 at episode 2194, time count 67000\n",
            "running reward: 130.34 at episode 2199, time count 68000\n",
            "running reward: 136.79 at episode 2205, time count 69000\n",
            "running reward: 141.60 at episode 2210, time count 70000\n",
            "running reward: 146.88 at episode 2215, time count 71000\n",
            "running reward: 151.64 at episode 2220, time count 72000\n",
            "running reward: 157.49 at episode 2226, time count 73000\n",
            "running reward: 161.95 at episode 2231, time count 74000\n",
            "running reward: 166.87 at episode 2236, time count 75000\n",
            "running reward: 170.87 at episode 2241, time count 76000\n",
            "running reward: 175.50 at episode 2247, time count 77000\n",
            "running reward: 177.99 at episode 2254, time count 78000\n",
            "running reward: 177.73 at episode 2261, time count 79000\n",
            "running reward: 171.00 at episode 2271, time count 80000\n",
            "running reward: 166.33 at episode 2280, time count 81000\n",
            "running reward: 163.31 at episode 2287, time count 82000\n",
            "running reward: 161.16 at episode 2294, time count 83000\n",
            "running reward: 153.76 at episode 2303, time count 84000\n",
            "running reward: 143.15 at episode 2314, time count 85000\n",
            "running reward: 134.82 at episode 2323, time count 86000\n",
            "running reward: 126.74 at episode 2333, time count 87000\n",
            "running reward: 119.04 at episode 2342, time count 88000\n",
            "running reward: 115.20 at episode 2350, time count 89000\n",
            "running reward: 113.52 at episode 2358, time count 90000\n",
            "running reward: 114.71 at episode 2366, time count 91000\n",
            "running reward: 115.46 at episode 2374, time count 92000\n",
            "running reward: 118.95 at episode 2381, time count 93000\n",
            "running reward: 118.05 at episode 2389, time count 94000\n",
            "running reward: 117.32 at episode 2395, time count 95000\n",
            "running reward: 120.03 at episode 2403, time count 96000\n",
            "running reward: 122.53 at episode 2411, time count 97000\n",
            "running reward: 124.37 at episode 2419, time count 98000\n",
            "running reward: 123.59 at episode 2429, time count 99000\n",
            "running reward: 125.59 at episode 2437, time count 100000\n",
            "running reward: 126.88 at episode 2445, time count 101000\n",
            "running reward: 126.85 at episode 2453, time count 102000\n",
            "running reward: 124.77 at episode 2462, time count 103000\n",
            "running reward: 123.50 at episode 2471, time count 104000\n",
            "running reward: 120.14 at episode 2481, time count 105000\n",
            "running reward: 109.02 at episode 2496, time count 106000\n",
            "running reward: 101.10 at episode 2510, time count 107000\n",
            "running reward: 99.81 at episode 2520, time count 108000\n",
            "running reward: 94.19 at episode 2534, time count 109000\n",
            "running reward: 79.80 at episode 2553, time count 110000\n",
            "running reward: 73.66 at episode 2568, time count 111000\n",
            "running reward: 54.92 at episode 2603, time count 112000\n",
            "running reward: 44.99 at episode 2627, time count 113000\n",
            "running reward: 48.81 at episode 2637, time count 114000\n",
            "running reward: 52.91 at episode 2647, time count 115000\n",
            "running reward: 58.00 at episode 2655, time count 116000\n",
            "running reward: 60.47 at episode 2666, time count 117000\n",
            "running reward: 68.00 at episode 2675, time count 118000\n",
            "running reward: 75.48 at episode 2683, time count 119000\n",
            "running reward: 83.04 at episode 2692, time count 120000\n",
            "running reward: 90.15 at episode 2701, time count 121000\n",
            "running reward: 97.43 at episode 2710, time count 122000\n",
            "running reward: 102.99 at episode 2720, time count 123000\n",
            "running reward: 105.96 at episode 2731, time count 124000\n",
            "running reward: 102.49 at episode 2744, time count 125000\n",
            "running reward: 99.13 at episode 2757, time count 126000\n",
            "running reward: 97.88 at episode 2768, time count 127000\n",
            "running reward: 94.82 at episode 2779, time count 128000\n",
            "running reward: 89.44 at episode 2793, time count 129000\n",
            "running reward: 87.85 at episode 2803, time count 130000\n",
            "running reward: 86.71 at episode 2814, time count 131000\n",
            "running reward: 86.34 at episode 2824, time count 132000\n",
            "running reward: 82.35 at episode 2840, time count 133000\n",
            "running reward: 81.97 at episode 2853, time count 134000\n",
            "running reward: 78.47 at episode 2870, time count 135000\n",
            "running reward: 77.99 at episode 2881, time count 136000\n",
            "running reward: 77.78 at episode 2895, time count 137000\n",
            "running reward: 75.58 at episode 2908, time count 138000\n",
            "running reward: 75.65 at episode 2918, time count 139000\n",
            "running reward: 76.84 at episode 2930, time count 140000\n",
            "running reward: 78.61 at episode 2941, time count 141000\n",
            "running reward: 79.36 at episode 2954, time count 142000\n",
            "running reward: 79.71 at episode 2971, time count 143000\n",
            "running reward: 75.63 at episode 2989, time count 144000\n",
            "running reward: 77.22 at episode 3001, time count 145000\n",
            "running reward: 76.34 at episode 3012, time count 146000\n",
            "running reward: 76.76 at episode 3022, time count 147000\n",
            "running reward: 78.07 at episode 3033, time count 148000\n",
            "running reward: 77.34 at episode 3045, time count 149000\n",
            "running reward: 78.25 at episode 3057, time count 150000\n",
            "running reward: 80.44 at episode 3069, time count 151000\n",
            "running reward: 81.86 at episode 3086, time count 152000\n",
            "running reward: 83.76 at episode 3097, time count 153000\n",
            "running reward: 83.03 at episode 3109, time count 154000\n",
            "running reward: 80.24 at episode 3122, time count 155000\n",
            "running reward: 80.19 at episode 3133, time count 156000\n",
            "running reward: 80.25 at episode 3145, time count 157000\n",
            "running reward: 80.70 at episode 3156, time count 158000\n",
            "running reward: 79.11 at episode 3170, time count 159000\n",
            "running reward: 81.55 at episode 3184, time count 160000\n",
            "running reward: 82.56 at episode 3194, time count 161000\n",
            "running reward: 80.97 at episode 3207, time count 162000\n",
            "running reward: 82.58 at episode 3219, time count 163000\n",
            "running reward: 82.28 at episode 3230, time count 164000\n",
            "running reward: 81.24 at episode 3243, time count 165000\n",
            "running reward: 80.26 at episode 3256, time count 166000\n",
            "running reward: 81.83 at episode 3268, time count 167000\n",
            "running reward: 80.99 at episode 3282, time count 168000\n",
            "running reward: 80.88 at episode 3293, time count 169000\n",
            "running reward: 81.14 at episode 3305, time count 170000\n",
            "running reward: 82.94 at episode 3315, time count 171000\n",
            "running reward: 84.71 at episode 3325, time count 172000\n",
            "running reward: 83.33 at episode 3338, time count 173000\n",
            "running reward: 86.37 at episode 3349, time count 174000\n",
            "running reward: 86.24 at episode 3361, time count 175000\n",
            "running reward: 85.92 at episode 3374, time count 176000\n",
            "running reward: 86.27 at episode 3386, time count 177000\n",
            "running reward: 84.82 at episode 3400, time count 178000\n",
            "running reward: 82.48 at episode 3413, time count 179000\n",
            "running reward: 79.60 at episode 3426, time count 180000\n",
            "running reward: 79.07 at episode 3439, time count 181000\n",
            "running reward: 76.61 at episode 3453, time count 182000\n",
            "running reward: 77.45 at episode 3464, time count 183000\n",
            "running reward: 76.81 at episode 3478, time count 184000\n",
            "running reward: 76.26 at episode 3491, time count 185000\n",
            "running reward: 74.65 at episode 3506, time count 186000\n",
            "running reward: 74.29 at episode 3520, time count 187000\n",
            "running reward: 76.68 at episode 3531, time count 188000\n",
            "running reward: 77.70 at episode 3544, time count 189000\n",
            "running reward: 77.28 at episode 3556, time count 190000\n",
            "running reward: 78.93 at episode 3566, time count 191000\n",
            "running reward: 80.48 at episode 3577, time count 192000\n",
            "running reward: 82.68 at episode 3588, time count 193000\n",
            "running reward: 85.11 at episode 3599, time count 194000\n",
            "running reward: 87.94 at episode 3609, time count 195000\n",
            "running reward: 89.69 at episode 3620, time count 196000\n",
            "running reward: 89.49 at episode 3633, time count 197000\n",
            "running reward: 90.85 at episode 3642, time count 198000\n",
            "running reward: 91.67 at episode 3653, time count 199000\n",
            "running reward: 93.24 at episode 3663, time count 200000\n",
            "running reward: 93.27 at episode 3673, time count 201000\n",
            "running reward: 94.67 at episode 3683, time count 202000\n",
            "running reward: 95.61 at episode 3693, time count 203000\n",
            "running reward: 97.86 at episode 3702, time count 204000\n",
            "running reward: 97.09 at episode 3713, time count 205000\n",
            "running reward: 97.98 at episode 3722, time count 206000\n",
            "running reward: 99.53 at episode 3733, time count 207000\n",
            "running reward: 99.87 at episode 3743, time count 208000\n",
            "running reward: 98.52 at episode 3755, time count 209000\n",
            "running reward: 97.59 at episode 3765, time count 210000\n",
            "running reward: 98.19 at episode 3775, time count 211000\n",
            "running reward: 97.92 at episode 3785, time count 212000\n",
            "running reward: 96.77 at episode 3796, time count 213000\n",
            "running reward: 95.62 at episode 3806, time count 214000\n",
            "running reward: 95.75 at episode 3817, time count 215000\n",
            "running reward: 95.62 at episode 3827, time count 216000\n",
            "running reward: 94.70 at episode 3838, time count 217000\n",
            "running reward: 94.96 at episode 3849, time count 218000\n",
            "running reward: 93.34 at episode 3862, time count 219000\n",
            "running reward: 92.45 at episode 3872, time count 220000\n",
            "running reward: 92.50 at episode 3882, time count 221000\n",
            "running reward: 93.39 at episode 3892, time count 222000\n",
            "running reward: 94.49 at episode 3902, time count 223000\n",
            "running reward: 95.83 at episode 3911, time count 224000\n",
            "running reward: 96.47 at episode 3921, time count 225000\n",
            "running reward: 96.57 at episode 3931, time count 226000\n",
            "running reward: 96.62 at episode 3942, time count 227000\n",
            "running reward: 96.66 at episode 3953, time count 228000\n",
            "running reward: 97.41 at episode 3965, time count 229000\n",
            "running reward: 97.89 at episode 3974, time count 230000\n",
            "running reward: 97.47 at episode 3984, time count 231000\n",
            "running reward: 98.92 at episode 3993, time count 232000\n",
            "running reward: 100.42 at episode 4001, time count 233000\n",
            "running reward: 100.30 at episode 4011, time count 234000\n",
            "running reward: 100.10 at episode 4021, time count 235000\n",
            "running reward: 100.26 at episode 4031, time count 236000\n",
            "running reward: 100.53 at episode 4041, time count 237000\n",
            "running reward: 101.76 at episode 4051, time count 238000\n",
            "running reward: 102.96 at episode 4061, time count 239000\n",
            "running reward: 105.72 at episode 4069, time count 240000\n",
            "running reward: 106.40 at episode 4078, time count 241000\n",
            "running reward: 107.17 at episode 4087, time count 242000\n",
            "running reward: 106.44 at episode 4096, time count 243000\n",
            "running reward: 106.10 at episode 4105, time count 244000\n",
            "running reward: 106.04 at episode 4114, time count 245000\n",
            "running reward: 107.86 at episode 4123, time count 246000\n",
            "running reward: 108.51 at episode 4132, time count 247000\n",
            "running reward: 109.04 at episode 4142, time count 248000\n",
            "running reward: 109.43 at episode 4152, time count 249000\n",
            "running reward: 109.75 at episode 4161, time count 250000\n",
            "running reward: 107.53 at episode 4171, time count 251000\n",
            "running reward: 107.08 at episode 4180, time count 252000\n",
            "running reward: 105.87 at episode 4190, time count 253000\n",
            "running reward: 106.07 at episode 4199, time count 254000\n",
            "running reward: 106.04 at episode 4209, time count 255000\n",
            "running reward: 106.20 at episode 4218, time count 256000\n",
            "running reward: 106.81 at episode 4225, time count 257000\n",
            "running reward: 107.77 at episode 4234, time count 258000\n",
            "running reward: 109.48 at episode 4243, time count 259000\n",
            "running reward: 112.26 at episode 4250, time count 260000\n",
            "running reward: 114.76 at episode 4257, time count 261000\n",
            "running reward: 116.57 at episode 4264, time count 262000\n",
            "running reward: 118.71 at episode 4272, time count 263000\n",
            "running reward: 119.95 at episode 4280, time count 264000\n",
            "running reward: 122.02 at episode 4288, time count 265000\n",
            "running reward: 124.07 at episode 4296, time count 266000\n",
            "running reward: 126.09 at episode 4302, time count 267000\n",
            "running reward: 129.22 at episode 4309, time count 268000\n",
            "running reward: 131.53 at episode 4316, time count 269000\n",
            "running reward: 133.38 at episode 4323, time count 270000\n",
            "running reward: 134.82 at episode 4330, time count 271000\n",
            "running reward: 134.49 at episode 4339, time count 272000\n",
            "running reward: 135.16 at episode 4346, time count 273000\n",
            "running reward: 134.60 at episode 4354, time count 274000\n",
            "running reward: 133.82 at episode 4361, time count 275000\n",
            "running reward: 134.25 at episode 4369, time count 276000\n",
            "running reward: 136.05 at episode 4375, time count 277000\n",
            "running reward: 137.47 at episode 4382, time count 278000\n",
            "running reward: 140.17 at episode 4388, time count 279000\n",
            "running reward: 142.51 at episode 4394, time count 280000\n",
            "running reward: 145.42 at episode 4399, time count 281000\n",
            "running reward: 147.89 at episode 4404, time count 282000\n",
            "running reward: 150.58 at episode 4409, time count 283000\n",
            "running reward: 153.35 at episode 4414, time count 284000\n",
            "running reward: 156.14 at episode 4419, time count 285000\n",
            "running reward: 157.63 at episode 4424, time count 286000\n",
            "running reward: 159.92 at episode 4430, time count 287000\n",
            "running reward: 163.48 at episode 4435, time count 288000\n",
            "running reward: 166.80 at episode 4441, time count 289000\n",
            "running reward: 169.47 at episode 4446, time count 290000\n",
            "running reward: 173.18 at episode 4451, time count 291000\n",
            "running reward: 176.95 at episode 4456, time count 292000\n",
            "running reward: 180.16 at episode 4461, time count 293000\n",
            "running reward: 183.66 at episode 4466, time count 294000\n",
            "running reward: 186.46 at episode 4471, time count 295000\n",
            "running reward: 188.44 at episode 4476, time count 296000\n",
            "Solved at episode 4479!\n"
          ]
        }
      ]
    }
  ]
}